{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import *\n",
    "from models import *\n",
    "import spacy\n",
    "\n",
    "\n",
    "def print_dict(dict_name, indent=2):\n",
    "    print(f\"{dict_name}: \\n\" + json.dumps(eval(dict_name),indent=indent))\n",
    "\n",
    "def print_var(var_name):\n",
    "    print(f\"{var_name}: {eval(var_name)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_example_cbow():\n",
    "    spacy_tokenizer = spacy.load('en')\n",
    "    \n",
    "    global example, preprocessed_example, tokenized_example\n",
    "    example = dict(\n",
    "        original_sent = \"What if <Sociologists/> Had as Much Influence as Economists ?\",\n",
    "        edit_word = \"donkeys\",\n",
    "    )\n",
    "    print_dict(\"example\")\n",
    "\n",
    "    i=example[\"original_sent\"]\n",
    "    old_word = re.findall(\"<(.*)/>\", i)[0]\n",
    "    l = re.findall(\"(.*)<.*/>(.*)\", i)[0]\n",
    "    l = [j.strip() for j in l]\n",
    "    context = \" \".join(l).strip()\n",
    "\n",
    "\n",
    "    preprocessed_example = dict(\n",
    "        original_word = old_word,\n",
    "        edit_word = example['edit_word'],\n",
    "        context_word = context, \n",
    "    )\n",
    "    print_dict(\"preprocessed_example\")\n",
    "\n",
    "\n",
    "    tokenized_example = {k: np.array(spacy_tokenizer(v)).astype('str').tolist() for k,v in preprocessed_example.items()}\n",
    "    print_dict(\"tokenized_example\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example: \n",
      "{\n",
      "  \"original_sent\": \"What if <Sociologists/> Had as Much Influence as Economists ?\",\n",
      "  \"edit_word\": \"donkeys\"\n",
      "}\n",
      "preprocessed_example: \n",
      "{\n",
      "  \"original_word\": \"Sociologists\",\n",
      "  \"edit_word\": \"donkeys\",\n",
      "  \"context_word\": \"What if Had as Much Influence as Economists ?\"\n",
      "}\n",
      "tokenized_example: \n",
      "{\n",
      "  \"original_word\": [\n",
      "    \"Sociologists\"\n",
      "  ],\n",
      "  \"edit_word\": [\n",
      "    \"donkeys\"\n",
      "  ],\n",
      "  \"context_word\": [\n",
      "    \"What\",\n",
      "    \"if\",\n",
      "    \"Had\",\n",
      "    \"as\",\n",
      "    \"Much\",\n",
      "    \"Influence\",\n",
      "    \"as\",\n",
      "    \"Economists\",\n",
      "    \"?\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# cbow example\n",
    "tokenize_example_cbow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_example_transformer(tokenizer, example):\n",
    "    \n",
    "    \n",
    "    global special_tokens, preprocessed_example, tokenized_example, idx_example\n",
    "    \n",
    "    print_dict(\"example\")\n",
    "\n",
    "\n",
    "    special_tokens = dict(\n",
    "        mask_token = tokenizer.mask_token,\n",
    "        sep_token = tokenizer.sep_token\n",
    "    )\n",
    "    print_dict(\"special_tokens\")\n",
    "\n",
    "\n",
    "    new_sent = re.sub(\"<.*/>\", f\"<{example['edit_word']}/>\", example[\"original_sent\"])\n",
    "    mask_sent = re.sub(\"<.*/>\", f\"<{tokenizer.mask_token}/>\",  example[\"original_sent\"])\n",
    "    preprocessed_example = dict(\n",
    "        original_sent = example[\"original_sent\"],\n",
    "        edit_sent = new_sent ,\n",
    "        mask_sent = mask_sent,\n",
    "    )\n",
    "    print_dict(\"preprocessed_example\")\n",
    "\n",
    "    \n",
    "    tokenized_example = {k: tokenize_and_cut(v, tokenizer) for k,v in preprocessed_example.items()}\n",
    "    print_dict(\"tokenized_example\")\n",
    "\n",
    "\n",
    "    idx_example = {k: tokenizer.convert_tokens_to_ids(v) for k,v in tokenized_example.items()}\n",
    "    print_var(\"tokenizer.sep_token_id\")\n",
    "    print_var(\"tokenizer.mask_token_id\")\n",
    "    print('')\n",
    "    print_dict(\"idx_example\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37bd326bc9724d82a202e034bdf08286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=481.0, style=ProgressStyle(description_â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load bert\n",
    "bert_tokenizer, bert = init_transformers('bert-base-uncased')\n",
    "\n",
    "# load roberta\n",
    "roberta_tokenizer, roberta = init_transformers('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example: \n",
      "{\n",
      "  \"original_sent\": \"What if <Sociologists/> Had as Much Influence as Economists ?\",\n",
      "  \"edit_word\": \"donkeys\"\n",
      "}\n",
      "special_tokens: \n",
      "{\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"sep_token\": \"[SEP]\"\n",
      "}\n",
      "preprocessed_example: \n",
      "{\n",
      "  \"original_sent\": \"What if <Sociologists/> Had as Much Influence as Economists ?\",\n",
      "  \"edit_sent\": \"What if <donkeys/> Had as Much Influence as Economists ?\",\n",
      "  \"mask_sent\": \"What if <[MASK]/> Had as Much Influence as Economists ?\"\n",
      "}\n",
      "tokenized_example: \n",
      "{\n",
      "  \"original_sent\": [\n",
      "    \"what\",\n",
      "    \"if\",\n",
      "    \"[SEP]\",\n",
      "    \"sociologist\",\n",
      "    \"##s\",\n",
      "    \"[SEP]\",\n",
      "    \"had\",\n",
      "    \"as\",\n",
      "    \"much\",\n",
      "    \"influence\",\n",
      "    \"as\",\n",
      "    \"economists\",\n",
      "    \"?\"\n",
      "  ],\n",
      "  \"edit_sent\": [\n",
      "    \"what\",\n",
      "    \"if\",\n",
      "    \"[SEP]\",\n",
      "    \"donkey\",\n",
      "    \"##s\",\n",
      "    \"[SEP]\",\n",
      "    \"had\",\n",
      "    \"as\",\n",
      "    \"much\",\n",
      "    \"influence\",\n",
      "    \"as\",\n",
      "    \"economists\",\n",
      "    \"?\"\n",
      "  ],\n",
      "  \"mask_sent\": [\n",
      "    \"what\",\n",
      "    \"if\",\n",
      "    \"[SEP]\",\n",
      "    \"[MASK]\",\n",
      "    \"[SEP]\",\n",
      "    \"had\",\n",
      "    \"as\",\n",
      "    \"much\",\n",
      "    \"influence\",\n",
      "    \"as\",\n",
      "    \"economists\",\n",
      "    \"?\"\n",
      "  ]\n",
      "}\n",
      "tokenizer.sep_token_id: 102\n",
      "tokenizer.mask_token_id: 103\n",
      "\n",
      "idx_example: \n",
      "{\n",
      "  \"original_sent\": [\n",
      "    2054,\n",
      "    2065,\n",
      "    102,\n",
      "    25106,\n",
      "    2015,\n",
      "    102,\n",
      "    2018,\n",
      "    2004,\n",
      "    2172,\n",
      "    3747,\n",
      "    2004,\n",
      "    22171,\n",
      "    1029\n",
      "  ],\n",
      "  \"edit_sent\": [\n",
      "    2054,\n",
      "    2065,\n",
      "    102,\n",
      "    20325,\n",
      "    2015,\n",
      "    102,\n",
      "    2018,\n",
      "    2004,\n",
      "    2172,\n",
      "    3747,\n",
      "    2004,\n",
      "    22171,\n",
      "    1029\n",
      "  ],\n",
      "  \"mask_sent\": [\n",
      "    2054,\n",
      "    2065,\n",
      "    102,\n",
      "    103,\n",
      "    102,\n",
      "    2018,\n",
      "    2004,\n",
      "    2172,\n",
      "    3747,\n",
      "    2004,\n",
      "    22171,\n",
      "    1029\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "example = dict(\n",
    "    original_sent = \"What if <Sociologists/> Had as Much Influence as Economists ?\",\n",
    "    edit_word = \"donkeys\",\n",
    ")\n",
    "\n",
    "# bert example, word level BPE\n",
    "tokenizer = bert_tokenizer\n",
    "tokenize_example_transformer(tokenizer, example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example: \n",
      "{\n",
      "  \"original_sent\": \"What if <Sociologists/> Had as Much Influence as Economists ?\",\n",
      "  \"edit_word\": \"donkeys\"\n",
      "}\n",
      "special_tokens: \n",
      "{\n",
      "  \"mask_token\": \"<mask>\",\n",
      "  \"sep_token\": \"</s>\"\n",
      "}\n",
      "preprocessed_example: \n",
      "{\n",
      "  \"original_sent\": \"What if <Sociologists/> Had as Much Influence as Economists ?\",\n",
      "  \"edit_sent\": \"What if <donkeys/> Had as Much Influence as Economists ?\",\n",
      "  \"mask_sent\": \"What if <<mask>/> Had as Much Influence as Economists ?\"\n",
      "}\n",
      "tokenized_example: \n",
      "{\n",
      "  \"original_sent\": [\n",
      "    \"What\",\n",
      "    \"\\u0120if\",\n",
      "    \"</s>\",\n",
      "    \"\\u0120Soci\",\n",
      "    \"ologists\",\n",
      "    \"</s>\",\n",
      "    \"\\u0120Had\",\n",
      "    \"\\u0120as\",\n",
      "    \"\\u0120Much\",\n",
      "    \"\\u0120Influence\",\n",
      "    \"\\u0120as\",\n",
      "    \"\\u0120Econom\",\n",
      "    \"ists\",\n",
      "    \"\\u0120?\"\n",
      "  ],\n",
      "  \"edit_sent\": [\n",
      "    \"What\",\n",
      "    \"\\u0120if\",\n",
      "    \"</s>\",\n",
      "    \"\\u0120don\",\n",
      "    \"keys\",\n",
      "    \"</s>\",\n",
      "    \"\\u0120Had\",\n",
      "    \"\\u0120as\",\n",
      "    \"\\u0120Much\",\n",
      "    \"\\u0120Influence\",\n",
      "    \"\\u0120as\",\n",
      "    \"\\u0120Econom\",\n",
      "    \"ists\",\n",
      "    \"\\u0120?\"\n",
      "  ],\n",
      "  \"mask_sent\": [\n",
      "    \"What\",\n",
      "    \"\\u0120if\",\n",
      "    \"</s>\",\n",
      "    \"<mask>\",\n",
      "    \"</s>\",\n",
      "    \"\\u0120Had\",\n",
      "    \"\\u0120as\",\n",
      "    \"\\u0120Much\",\n",
      "    \"\\u0120Influence\",\n",
      "    \"\\u0120as\",\n",
      "    \"\\u0120Econom\",\n",
      "    \"ists\",\n",
      "    \"\\u0120?\"\n",
      "  ]\n",
      "}\n",
      "tokenizer.sep_token_id: 2\n",
      "tokenizer.mask_token_id: 50264\n",
      "\n",
      "idx_example: \n",
      "{\n",
      "  \"original_sent\": [\n",
      "    2264,\n",
      "    114,\n",
      "    2,\n",
      "    35160,\n",
      "    10974,\n",
      "    2,\n",
      "    7301,\n",
      "    25,\n",
      "    7840,\n",
      "    41284,\n",
      "    25,\n",
      "    17833,\n",
      "    1952,\n",
      "    17487\n",
      "  ],\n",
      "  \"edit_sent\": [\n",
      "    2264,\n",
      "    114,\n",
      "    2,\n",
      "    218,\n",
      "    18161,\n",
      "    2,\n",
      "    7301,\n",
      "    25,\n",
      "    7840,\n",
      "    41284,\n",
      "    25,\n",
      "    17833,\n",
      "    1952,\n",
      "    17487\n",
      "  ],\n",
      "  \"mask_sent\": [\n",
      "    2264,\n",
      "    114,\n",
      "    2,\n",
      "    50264,\n",
      "    2,\n",
      "    7301,\n",
      "    25,\n",
      "    7840,\n",
      "    41284,\n",
      "    25,\n",
      "    17833,\n",
      "    1952,\n",
      "    17487\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "example = dict(\n",
    "    original_sent = \"What if <Sociologists/> Had as Much Influence as Economists ?\",\n",
    "    edit_word = \"donkeys\",\n",
    ")\n",
    "\n",
    "# roberta example, byte level BPE\n",
    "tokenizer = roberta_tokenizer\n",
    "tokenize_example_transformer(tokenizer, example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_tokenizer.pretrained_vocab_files_map\n",
    "# bert_vocab = bert_tokenizer.get_vocab()\n",
    "# len(bert_vocab)\n",
    "\n",
    "# roberta_tokenizer.pretrained_vocab_files_map\n",
    "# roberta_vocab = roberta_tokenizer.get_vocab()\n",
    "# len(roberta_vocab)\n",
    "# roberta_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [\n",
    "    \"what\",\n",
    "    \"if\",\n",
    "    \"[SEP]\",\n",
    "    \"sociologist\",\n",
    "    \"##s\",\n",
    "    \"[SEP]\",\n",
    "    \"had\",\n",
    "    \"as\",\n",
    "    \"much\",\n",
    "    \"influence\",\n",
    "    \"as\",\n",
    "    \"economists\",\n",
    "    \"?\"\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example: \n",
      "{\n",
      "  \"original_sent\": \"California and <President Trump/> are going to war with each other\",\n",
      "  \"edit_word\": \"monkeys\"\n",
      "}\n",
      "special_tokens: \n",
      "{\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"sep_token\": \"[SEP]\"\n",
      "}\n",
      "preprocessed_example: \n",
      "{\n",
      "  \"original_sent\": \"California and <President Trump/> are going to war with each other\",\n",
      "  \"edit_sent\": \"California and <monkeys/> are going to war with each other\",\n",
      "  \"mask_sent\": \"California and <[MASK]/> are going to war with each other\"\n",
      "}\n",
      "tokenized_example: \n",
      "{\n",
      "  \"original_sent\": [\n",
      "    \"california\",\n",
      "    \"and\",\n",
      "    \"[SEP]\",\n",
      "    \"president\",\n",
      "    \"trump\",\n",
      "    \"[SEP]\",\n",
      "    \"are\",\n",
      "    \"going\",\n",
      "    \"to\",\n",
      "    \"war\",\n",
      "    \"with\",\n",
      "    \"each\",\n",
      "    \"other\"\n",
      "  ],\n",
      "  \"edit_sent\": [\n",
      "    \"california\",\n",
      "    \"and\",\n",
      "    \"[SEP]\",\n",
      "    \"monkeys\",\n",
      "    \"[SEP]\",\n",
      "    \"are\",\n",
      "    \"going\",\n",
      "    \"to\",\n",
      "    \"war\",\n",
      "    \"with\",\n",
      "    \"each\",\n",
      "    \"other\"\n",
      "  ],\n",
      "  \"mask_sent\": [\n",
      "    \"california\",\n",
      "    \"and\",\n",
      "    \"[SEP]\",\n",
      "    \"[MASK]\",\n",
      "    \"[SEP]\",\n",
      "    \"are\",\n",
      "    \"going\",\n",
      "    \"to\",\n",
      "    \"war\",\n",
      "    \"with\",\n",
      "    \"each\",\n",
      "    \"other\"\n",
      "  ]\n",
      "}\n",
      "tokenizer.sep_token_id: 102\n",
      "tokenizer.mask_token_id: 103\n",
      "\n",
      "idx_example: \n",
      "{\n",
      "  \"original_sent\": [\n",
      "    2662,\n",
      "    1998,\n",
      "    102,\n",
      "    2343,\n",
      "    8398,\n",
      "    102,\n",
      "    2024,\n",
      "    2183,\n",
      "    2000,\n",
      "    2162,\n",
      "    2007,\n",
      "    2169,\n",
      "    2060\n",
      "  ],\n",
      "  \"edit_sent\": [\n",
      "    2662,\n",
      "    1998,\n",
      "    102,\n",
      "    17059,\n",
      "    102,\n",
      "    2024,\n",
      "    2183,\n",
      "    2000,\n",
      "    2162,\n",
      "    2007,\n",
      "    2169,\n",
      "    2060\n",
      "  ],\n",
      "  \"mask_sent\": [\n",
      "    2662,\n",
      "    1998,\n",
      "    102,\n",
      "    103,\n",
      "    102,\n",
      "    2024,\n",
      "    2183,\n",
      "    2000,\n",
      "    2162,\n",
      "    2007,\n",
      "    2169,\n",
      "    2060\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "example = dict(\n",
    "    original_sent = \"California and <President Trump/> are going to war with each other\",\n",
    "    edit_word = \"monkeys\",\n",
    ")\n",
    "\n",
    "# roberta example, byte level BPE\n",
    "tokenizer = bert_tokenizer\n",
    "tokenize_example_transformer(tokenizer, example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
