{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torchtext import data\n",
    "from torchtext.vocab import Vectors\n",
    "from torch.nn import init\n",
    "from tqdm import tqdm\n",
    "from torchtext.vocab import GloVe\n",
    "from torchtext import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### my dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(csv_data, id_field, text_field, label_field, is_final_valid = False):\n",
    "    fields = [('id', id_field), ('new', text_field), ('original2', text_field), ('meanGrade', label_field)]\n",
    "    fields2 = [('id', id_field), ('new', text_field), ('original2', text_field)]\n",
    "    examples = []\n",
    "    if is_final_valid:\n",
    "        for myid, new, original2 in tqdm(zip(csv_data['id'], csv_data['new'],csv_data['original2'])):\n",
    "            examples.append(data.Example.fromlist([myid, new, original2], fields2))\n",
    "        return examples, fields2\n",
    "    else:\n",
    "        for myid, new, original2, label in tqdm(zip(csv_data['id'], csv_data['new'], csv_data['original2'], csv_data['meanGrade'])):\n",
    "            examples.append(data.Example.fromlist([myid, new, original2, label], fields))\n",
    "        return examples, fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize = lambda x: x.split()\n",
    "TEXT = data.RawField()\n",
    "LABEL = data.LabelField(use_vocab=False, dtype=torch.float)\n",
    "ID = data.LabelField(use_vocab=False)\n",
    "\n",
    "train_path = \"../data/task-1/train2.csv\"\n",
    "valid_path = \"../data/task-1/dev2.csv\"\n",
    "test_path = \"../data/task-1/test2.csv\"\n",
    "    \n",
    "train = pd.read_csv(train_path)\n",
    "valid = pd.read_csv(valid_path)\n",
    "test = pd.read_csv(test_path)\n",
    "\n",
    "test = test.reset_index(drop=True)\n",
    "valid = valid.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9652it [00:00, 135126.71it/s]\n",
      "2419it [00:00, 116428.23it/s]\n",
      "3024it [00:00, 118040.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 9652\n",
      "Number of validation examples: 2419\n",
      "Number of testing examples: 3024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_examples, train_fields = get_dataset(train, ID, TEXT, LABEL)\n",
    "valid_examples, valid_fields = get_dataset(valid, ID, TEXT, LABEL)\n",
    "#test_examples, test_fields = get_dataset(test, TEXT, None, True)\n",
    "test_examples, test_fields = get_dataset(test, ID, TEXT, LABEL)\n",
    "#final_valid_examples, final_valid_fields = get_dataset(final_valid, ID, TEXT, LABEL, True)\n",
    "\n",
    "\n",
    "train_data = data.Dataset(train_examples, train_fields)\n",
    "valid_data = data.Dataset(valid_examples, valid_fields)\n",
    "test_data = data.Dataset(test_examples, test_fields)\n",
    "#final_valid_data = data.Dataset(final_valid_examples, final_valid_fields)\n",
    "\n",
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')\n",
    "#print(f'Number of final_valid examples: {len(final_valid_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in LABEL vocabulary: 33\n"
     ]
    }
   ],
   "source": [
    "# TEXT.build_vocab(train_data,vectors=\"glove.840B.300d\", unk_init=torch.Tensor.normal_) \n",
    "# TEXT.build_vocab(train_data) \n",
    "LABEL.build_vocab(train_data)\n",
    "# print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### baches example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.new), # the BucketIterator needs to be told what function it should use to group the data.\n",
    "    sort_within_batch=False,\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elmo_BiLSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, output_dim, hidden_dim, dropout):\n",
    "        super().__init__()\n",
    "        #self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True) \n",
    "        self.fc = nn.Linear(EMBEDDING_DIM*4, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, q2, q3):\n",
    "        # embedding[sent_len, betch_size, embedding_dim]\n",
    "        \n",
    "        # text[sent_len, batch_size]\n",
    "        # embedded = self.embedding()\n",
    "        # embeded[sent_len, batch_size, embedding_dim]\n",
    "        m = [q2, q3, (q2-q3).abs(), q2*q3]\n",
    "        pair_emb = torch.cat(m, dim=-1)\n",
    "        #print(q2.size())\n",
    "        #print(pair_emb.size())\n",
    "        # hidden[batch_size, hidden_dim * num_directions]\n",
    "        return self.fc(pair_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 1024\n",
    "OUTPUT_DIM = 1 # Classification: num_labels/Regression: 1\n",
    "HIDDEN_SIZE = 16 \n",
    "DROPOUT = 0.5 \n",
    "\n",
    "model = BiLSTMModel(EMBEDDING_DIM, OUTPUT_DIM, HIDDEN_SIZE, DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 4,097 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.MSELoss(reduction='sum') # TODO\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "options_file = \"https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
    "weight_file = \"https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
    "\n",
    "elmo = Elmo(options_file, weight_file, 2, dropout=0)\n",
    "\n",
    "def get_elmo_embeddings(batch):\n",
    "    sentences = [sen.split()for sen in batch]\n",
    "    length = torch.Tensor([len(sentences[i]) for i in range(len(sentences))])\n",
    "    character_ids = batch_to_ids(sentences)\n",
    "    embeddings = elmo(character_ids)\n",
    "    #embedding = embeddings['elmo_representations'][0].permute(1,2,0).max(dim=0)[0]\n",
    "    embedding = embeddings['elmo_representations'][0].permute(1,2,0)[0]\n",
    "    embedding = torch.div(embedding, length).permute(1,0)\n",
    "    ### embedding [sent_len, batch_size, embed_dim] -> embedding [batch_size, embed_dim]\n",
    "    return embedding\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        e1 = get_elmo_embeddings(batch.new).to(device)\n",
    "        e2 = get_elmo_embeddings(batch.original2).to(device)\n",
    "        predictions = model(e1,e2).squeeze(1)\n",
    "        loss = criterion(predictions, batch.meanGrade)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            e1 = get_elmo_embeddings(batch.new).to(device)\n",
    "            e2 = get_elmo_embeddings(batch.original2).to(device)\n",
    "            predictions = model(e1,e2).squeeze(1)\n",
    "            #predictions = model(embeddings).squeeze(1)\n",
    "            loss = criterion(predictions, batch.meanGrade)\n",
    "        \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    trainModel(model, train_iterator, optimizer, criterion)\n",
    "    \n",
    "    train_loss = trainModel(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'lstm-model.pth')\n",
    "        print(\"save\")\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(f'\\tValid Loss: {valid_loss:.3f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def myMSE(model, iterator):\n",
    "    pred_list = np.array([])\n",
    "    real_list = np.array([])\n",
    "    id_list = np.array([])\n",
    "    \n",
    "#     Result_Dict = sorted(list(set(train['meanGrade'])))\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            e1 = get_elmo_embeddings(batch.new).to(device)\n",
    "            e2 = get_elmo_embeddings(batch.original2).to(device)\n",
    "            predictions = model(e1,e2).squeeze(1)\n",
    "            pred = np.array(predictions.data.tolist())\n",
    "            real = np.array(batch.meanGrade.data.tolist())\n",
    "            myid = np.array(batch.id.data.tolist())\n",
    "            real_list = np.append(real_list, real)\n",
    "            pred_list = np.append(pred_list, pred)\n",
    "            id_list = np.append(id_list, myid)\n",
    "\n",
    "    \n",
    "# #     csv['pred_label'] = pred_list.round(0).astype(int)\n",
    "#     csv['pred'] = [Result_Dict[i] for i in csv['pred_label']]\n",
    "\n",
    "\n",
    "    df = pd.DataFrame({'id':id_list, 'real':real_list, 'pred':pred_list})\n",
    "    rmse = np.sqrt(np.mean((df['real'] - df['pred'])**2))\n",
    "            \n",
    "    print(rmse)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试 best的model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSEPlus():\n",
    "    \"\"\"\n",
    "    for full: RMSE\n",
    "    for top n% + bottom n%: RMSE@10, RMSE@20, RMSE@30, RMSE@40\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.pred_list = []\n",
    "        self.real_list = []\n",
    "\n",
    "    def __call__(self, predictions, labels):\n",
    "        if isinstance(predictions, torch.Tensor):\n",
    "            #predictions = predictions.detach().cpu().numpy()\n",
    "            predictions = predictions.data.tolist()\n",
    "        if isinstance(labels, torch.Tensor):\n",
    "            #labels = labels.detach().cpu().numpy()\n",
    "            labels = labels.data.tolist()\n",
    "\n",
    "        self.real_list += labels\n",
    "        self.pred_list += predictions\n",
    "\n",
    "    def get_metric(self, reset=False):\n",
    "        metrics = {}\n",
    "        df = pd.DataFrame({'real': self.real_list, 'pred': self.pred_list})\n",
    "        metrics['rmse'] = np.sqrt(np.mean((df['real'] - df['pred']) ** 2))\n",
    "\n",
    "        df = df.sort_values(by=['real'], ascending=False)\n",
    "        for percent in [10, 20, 30, 40]:\n",
    "            size = math.ceil(len(df) * percent * 0.01)\n",
    "            # top n % + bottom n %\n",
    "            df2 = df[:size].append(df[-size:])\n",
    "            rmse = np.sqrt(np.mean((df2['real'] - df2['pred'])**2))\n",
    "            metrics[f'rmse_{percent}'] = rmse\n",
    "        if reset:\n",
    "            self.reset()\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def reset(self):\n",
    "        self.pred_list = []\n",
    "        self.real_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"lstm-model.pth\"))\n",
    "#df = myMSE(model, train_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5622185554445007\n"
     ]
    }
   ],
   "source": [
    "df = myMSE(model, valid_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.654896035063133\n"
     ]
    }
   ],
   "source": [
    "df = myMSE(model, test_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "test = RMSEPlus()\n",
    "m = test.get_metric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
